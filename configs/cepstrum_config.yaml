datasets_path: ./Datasets
random_seed: 230

rir_duration: 32767
nfft: 16383
nwins: 16
min_early_reverb: 0.007 # minimum permitted early reverb time for considered RIRs(seconds). The early reverb is taken to be the time where the RIR is at its maximum value.
prealign_rir: False
sample_rate: 16000
same_batch_rir: True

data_in_ram: False

Encoding: # echo encoding params
  delays: [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100] 
  amplitude: 0.8
  win_size: 2048
  kernel: "bp"
  decoding: "cepstrum" # options: "autocepstrum", "cepstrum"
  cutoff_freq: 1000

Model:
  model_name: EchoSpeechDAREUnet
  learning_rate: 0.00001
  nwins: 16
  use_transformer: True
  alphas: [1, 0, 0.5, 0.5] # cepstrum MSE loss weight, symbol error rate weight, error reduction weight, intra-batch RIR diff loss
  softargmax_beta : 10000000
  residual: True

DataLoader:
  batch_size: 64
  num_workers: 16
  persistent_workers: True
  shuffle: True
  drop_last: True
  pin_memory: True

ModelCheckpoint:
  monitor: val_loss
  dirpath: cepstrum_checkpoints

LearningRateMonitor:
  logging_interval: epoch

DDPStrategy:
  process_group_backend: gloo
  find_unused_parameters: False

AdvancedProfiler:
  dirpath: null
  filename: advanced_profiler.txt
  line_count_restriction: 1.0

Trainer:
  limit_train_batches: 1000
  limit_val_batches: 3
  limit_test_batches: 111
  max_epochs: 100
  num_sanity_val_steps: 1
  accumulate_grad_batches: 1
  log_every_n_steps: 50 # logs every n steps where n is the number of batches
  # val_check_interval: 1 # val every this fraction of an epoch
  check_val_every_n_epoch: 1
  accelerator: gpu
  devices: [0]

