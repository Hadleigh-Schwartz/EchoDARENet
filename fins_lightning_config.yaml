sample_rate: &sr 48000 

# dare stuff, some redundancy
datasets_path: ./Datasets
preencoded_speech: True
random_seed: 230
nfft: 16383
nwins: 64
min_early_reverb: 0.007 # minimum permitted early reverb time for considered RIRs(seconds). The early reverb is taken to be the time where the RIR is at its maximum value.
prealign_rir: False
plot_every_n_steps: 50
norm_cepstra: True
cep_target_region: [30, 150] # start and end (samples) of region of the cepstrum to consider in computing the cepstrum loss # NOTE: Should be related to delays below
data_in_ram: False

Encoding: # echo encoding params
  # note: the below delays have been adapted to the 48kHz sampling rate, as opposed to the previously used 16khz
  delays: [40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148]
  amplitude: 0.8
  win_size: 2048
  kernel: "bp"
  decoding: "cepstrum" # options: "autocepstrum", "cepstrum"
  cutoff_freq: 1000

# fins thing
dataset:
  params:
    sr: *sr
    input_duration: 2.74
    input_length : &input_length 131072
    rir_duration: &rir_duration 1.0
    direct_delay : 100
    peak_norm_value : &peak_norm_value 0.5
    source_norm_value : -25.0

# fins thing
model:
  name: "model.FilteredNoiseShaper"
  params: 
    num_filters : &num_filters 10
    filter_order : 1023
    sr: *sr
    rir_duration: *rir_duration
    input_length : *input_length
    early_length : &early_length 2400
    decoder_input_length : 400
    noise_condition_length : &noise_condition_length 16
    z_size : 128
    min_snr : 15
    max_snr : 50
    normalize : "rms"
    rms_level : &rms_level 0.01 

# dare thing, some redundancy
DataLoader:
  batch_size: 24
  num_workers: 16
  persistent_workers: True
  shuffle: True
  drop_last: True
  pin_memory: True

# fins thing
train:
  params:
    sr: *sr
    batch_size: 24
    num_workers: 1
    lr: 0.000055
    lr_step_size : 80
    lr_decay_factor : 0.8
    gradient_clip_value: 5
    num_epochs : 5000
    validation_interval: 1
    evaluation_interval : 10
    checkpoint_interval: 1
    random_plot_interval : 1
    logging_dir : "logs"
    checkpoint_dir : "checkpoints"
    early_length: *early_length
    peak_norm_value: *peak_norm_value
    input_length: *input_length
    rms_level: *rms_level
    rir_duration: *rir_duration
    noise_condition_length : *noise_condition_length
    num_filters : *num_filters

# fins thing
eval:
  params : 
    sr : *sr 
    rms_level : *rms_level
    noise_condition_length : *noise_condition_length
    num_filters: *num_filters
    input_length : *input_length

# dare thing, some redundancy
Trainer:
  limit_train_batches: 10000
  limit_val_batches: 100
  limit_test_batches: 100
  max_epochs: 100
  num_sanity_val_steps: 1
  accumulate_grad_batches: 1
  log_every_n_steps: 10 # logs every n steps where n iss the number of batches
  # val_check_interval: 0.05 # val every this fraction of an epoch
  check_val_every_n_epoch: 1
  accelerator: gpu
  devices: [1]

ModelCheckpoint:
  dirpath: fins_checkpoints
  every_n_train_steps: 1000


AdvancedProfiler:
  dirpath: null
  filename: advanced_profiler.txt
  line_count_restriction: 1.0
