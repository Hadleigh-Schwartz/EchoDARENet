sample_rate: 44100 # note: any loaded IR and speech data will be resampled to this rate
datasets_path: ./Datasets 
speech_dataset: "HiFi" # options: "LibriSpeech", "HiFi"
# TODO: RIR dataset choice
preencoded_speech: True # tells dataloader not to encode on the fly, as audio being loaded is already encoded
preencoded_speech_path: "preencoded_hifi" # relative to datasets_path
random_seed: 230
nwins: 36 # should be 64 @ 48khz to match original FINS.
min_early_reverb: 0.007 # minimum permitted early reverb time for considered RIRs(seconds). The early reverb is taken to be the time where the RIR is at its maximum value.
plot_every_n_steps: 50
norm_cepstra: True
cep_target_region: [30, 150] # start and end (samples) of region of the cepstrum to consider in computing the cepstrum loss # NOTE: Should be related to delays below
data_in_ram: False

Encoding: # echo encoding params
  # note: the below delays have been adapted to the 48kHz sampling rate, as opposed to the previously used 16khz
  delays: [40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148]
  amplitude: 0.8
  win_size: 2048
  kernel: "bp"
  decoding: "cepstrum" # options: "autocepstrum", "cepstrum"
  cutoff_freq: 1000
  hanning_factor: 2

fins:
  rir_duration: 1.0
  direct_delay : 100
  peak_norm_value : 0.5
  source_norm_value : -25.0
  num_filters : 10
  filter_order : 1023
  early_length : 2400
  decoder_input_length : 400
  noise_condition_length : 16
  z_size : 128
  min_snr : 15
  max_snr : 50
  normalize : "rms"
  rms_level : 0.01 
  gradient_clip_value : 5
  lr: 0.000055
  lr_step_size : 80
  lr_decay_factor : 0.8

DataLoader:
  batch_size: 24
  num_workers: 16
  persistent_workers: True
  shuffle: True
  drop_last: True
  pin_memory: True

Trainer:
  limit_train_batches: 10000
  limit_val_batches: 100
  limit_test_batches: 100
  max_epochs: 100
  num_sanity_val_steps: 1
  accumulate_grad_batches: 1
  log_every_n_steps: 10 # logs every n steps where n iss the number of batches
  # val_check_interval: 0.05 # val every this fraction of an epoch
  check_val_every_n_epoch: 1
  accelerator: gpu
  devices: [1]

ModelCheckpoint:
  dirpath: fins_checkpoints
  every_n_train_steps: 1000

AdvancedProfiler:
  dirpath: null
  filename: advanced_profiler.txt
  line_count_restriction: 1.0
