sample_rate: &sr 48000 

# dare stuff, some redundancy
datasets_path: ./Datasets
random_seed: 230
nfft: 16383
nwins: 64
min_early_reverb: 0.007 # minimum permitted early reverb time for considered RIRs(seconds). The early reverb is taken to be the time where the RIR is at its maximum value.
prealign_rir: False
plot_every_n_steps: 50
norm_cepstra: True
cep_target_region: [30, 150] # start and end (samples) of region of the cepstrum to consider in computing the cepstrum loss # NOTE: Should be related to delays below
data_in_ram: False


Encoding: # echo encoding params
  delays: [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100] 
  amplitude: 0.8
  win_size: 2048
  kernel: "bp"
  decoding: "cepstrum" # options: "autocepstrum", "cepstrum"
  cutoff_freq: 1000

dataset:
  params:
    sr: *sr
    input_duration: 2.74
    input_length : &input_length 131072
    rir_duration: &rir_duration 1.0
    direct_delay : 100
    peak_norm_value : &peak_norm_value 0.5
    source_norm_value : -25.0

model:
  name: "model.FilteredNoiseShaper"
  params: 
    num_filters : &num_filters 10
    filter_order : 1023
    sr: *sr
    rir_duration: *rir_duration
    input_length : *input_length
    early_length : &early_length 2400
    decoder_input_length : 400
    noise_condition_length : &noise_condition_length 16
    z_size : 128
    min_snr : 15
    max_snr : 50
    normalize : "rms"
    rms_level : &rms_level 0.01 

# dare thing, some redundancy
DataLoader:
  batch_size: 16
  num_workers: 16
  persistent_workers: True
  shuffle: True
  drop_last: True
  pin_memory: True

train:
  params:
    sr: *sr
    batch_size: 16
    num_workers: 1
    lr: 0.000055
    lr_step_size : 80
    lr_decay_factor : 0.8
    gradient_clip_value: 5
    num_epochs : 5000
    validation_interval: 1
    evaluation_interval : 10
    checkpoint_interval: 1
    random_plot_interval : 1
    logging_dir : "logs"
    checkpoint_dir : "checkpoints"
    early_length: *early_length
    peak_norm_value: *peak_norm_value
    input_length: *input_length
    rms_level: *rms_level
    rir_duration: *rir_duration
    noise_condition_length : *noise_condition_length
    num_filters : *num_filters

eval:
  params : 
    sr : *sr 
    rms_level : *rms_level
    noise_condition_length : *noise_condition_length
    num_filters: *num_filters
    input_length : *input_length

# dare thing, some redundancy
Trainer:
  limit_train_batches: 20000
  limit_val_batches: 3
  limit_test_batches: 111
  max_epochs: 100
  num_sanity_val_steps: 1
  accumulate_grad_batches: 1
  log_every_n_steps: 10 # logs every n steps where n iss the number of batches
  val_check_interval: 0.05 # val every this fraction of an epoch
  check_val_every_n_epoch: 1
  accelerator: gpu
  devices: [1]
