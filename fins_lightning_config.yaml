sample_rate: 44100 # note: any loaded IR and speech data will be resampled to this rate
datasets_path: ./Datasets 
speech_dataset: "HiFi" # options: "LibriSpeech", "HiFi". Currently just used by preencode_speech.py. TODO: incorporate for non-preencodedkmJU
rir_dataset: "MIT" # options: HOM (homula IR dataset), MIT, sim
preencoded_speech: False # tells dataloader not to encode on the fly, as audio being loaded is already encoded
preencoded_speech_path: "preencoded_hifi" # relative to datasets_path
random_seed: 230
nwins: 36 # the model input size = nwins * win_size. Note: original FINS uses 131072 samples of input at 48 kHz
min_early_reverb: 0 # minimum permitted early reverb time for considered RIRs(seconds). The early reverb is taken to be the time where the RIR is at its maximum value.
plot_every_n_steps: 50
data_in_ram: False

Encoding: # echo encoding params
  # note: the below delays have been adapted to the 48kHz sampling rate, as opposed to the previously used 16khz
  delays: [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
  amplitude: 0.4
  win_size: 3000
  kernel: "bp"
  decoding: "cepstrum" # options: "autocepstrum", "cepstrum"
  cutoff_freq: 1000
  hanning_factor: 2

fins:
  rir_duration: 1.0
  direct_delay : 100
  peak_norm_value : 0.5
  source_norm_value : -25.0
  num_filters : 10
  filter_order : 1023
  early_length : 2400
  decoder_input_length : 368 # 400 is used by original FINS (assumes 48kz data)
  noise_condition_length : 16
  z_size : 128
  min_snr : 15
  max_snr : 50
  normalize : "rms"
  rms_level : 0.01 
  gradient_clip_value : 5
  lr: 0.000055
  lr_step_size : 80
  lr_decay_factor : 0.8

dare:
  learning_rate: 0.0001 
  use_transformer: True
  alphas: [1, 1, 0, 0] # cepstrum MSE loss weight, symbol error rate weight, error reduction weight, intra-batch RIR diff loss
  softargmax_beta : 10000000
  residual: True
  norm_cepstra: True
  cep_target_region: [50, 125] # start and end (samples) of region of the cepstrum to consider in computing the cepstrum loss # NOTE: Should be related to delays below



DataLoader:
  batch_size: 24
  num_workers: 16
  persistent_workers: True
  shuffle: True
  drop_last: True
  pin_memory: True

Trainer:
  limit_train_batches: 10000
  limit_val_batches: 100
  limit_test_batches: 100
  max_epochs: 100
  num_sanity_val_steps: 1
  accumulate_grad_batches: 1
  log_every_n_steps: 10 # logs every n steps where n iss the number of batches
  # val_check_interval: 0.05 # val every this fraction of an epoch
  check_val_every_n_epoch: 1
  accelerator: gpu
  devices: [1]

ModelCheckpoint:
  dirpath: fins_checkpoints
  every_n_train_steps: 1000

AdvancedProfiler:
  dirpath: null
  filename: advanced_profiler.txt
  line_count_restriction: 1.0

LearningRateMonitor:
  logging_interval: epoch

DDPStrategy:
  process_group_backend: gloo
  find_unused_parameters: False